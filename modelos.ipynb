{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de archivo gracias a la librería pandas\n",
    "archivo = pd.read_csv(\"alzheimers_disease_data.csv\")\n",
    "archivo = archivo.drop(\"DoctorInCharge\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Naive bayes Gauss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "\n",
    "#División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Balancear el conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Entrenamiento del modelo\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train_balanced) \n",
    "\n",
    "y_pred = nb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_NB = accuracy_score(y_test, y_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred,labels=[1,0] )\n",
    "\n",
    "#Reporte\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(class_report)\n",
    "\n",
    "#Matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "           xticklabels=['Positivo', 'Negativo'],\n",
    "           yticklabels=['Positivo', 'Negativo'])\n",
    "plt.title('Matriz de Confusión - GaussianNB')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "precision_NB = precision_score(y_test, y_pred)\n",
    "recall_NB = recall_score(y_test, y_pred)\n",
    "f1_nb = f1_score(y_test, y_pred)\n",
    "tn_nb, fp_nb, fn_nb, tp_nb = conf_matrix.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Naive Bayes Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "\n",
    "#División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Balancear el conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Entrenar modelo.\n",
    "Bernoulli_model = BernoulliNB()\n",
    "\n",
    "Bernoulli_model.fit(X_train_scaled, y_train_balanced) \n",
    "\n",
    "y_pred = Bernoulli_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_BNB = accuracy_score(y_test, y_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "#Reporte de clasificación\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(class_report)\n",
    "\n",
    "#Matriz de confusión con orden modificado\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Positivo', 'Negativo'],  \n",
    "            yticklabels=['Positivo', 'Negativo'])  \n",
    "plt.title('Matriz de Confusión - Bernoulli')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "precision_BNB = precision_score(y_test, y_pred)\n",
    "recall_BNB = recall_score(y_test, y_pred)\n",
    "f1_bnb = f1_score(y_test, y_pred)\n",
    "tn_bnb, fp_bnb, fn_bnb, tp_bnb = conf_matrix.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Naive Bayes Multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "\n",
    "#División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Balancear el conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Escalar datos\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Entrenar modelo\n",
    "multinomial_model = MultinomialNB()\n",
    "multinomial_model.fit(X_train_scaled, y_train_balanced) \n",
    "\n",
    "y_pred = multinomial_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_multinomial = accuracy_score(y_test, y_pred)\n",
    "print(accuracy_multinomial)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "#Reporte de clasificación\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(class_report)\n",
    "\n",
    "#Matriz de confusión con orden modificado\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Positivo', 'Negativo'],  \n",
    "            yticklabels=['Positivo', 'Negativo'])  \n",
    "plt.title('Matriz de Confusión - Multinomial')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "precision_multinomial = precision_score(y_test, y_pred)\n",
    "recall_multinomial = recall_score(y_test, y_pred)\n",
    "f1_multinomial = f1_score(y_test, y_pred)\n",
    "tn_multinomial, fp_multinomial, fn_multinomial, tp_multinomial = conf_matrix.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "#Separar datos de entrenamiento y de testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Balancear conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Estandarizar data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "k_range = range(1, 51)\n",
    "scores = []\n",
    "\n",
    "#Buscar mejor \"n\" y guardar sus valores para posteriormente graficarlos\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred_k = knn.predict(X_test_scaled)\n",
    "    scores.append(accuracy_score(y_test, y_pred_k))\n",
    "\n",
    "#Muestra en un gráfico el desempeño de KNN (MEJORADO)\n",
    "plt.figure(figsize=(20, 8))  # Tamaño más manejable\n",
    "plt.plot(k_range, scores, marker='o', linewidth=2, markersize=4)\n",
    "plt.title('Exactitud del modelo para diferentes valores de k', fontsize=14)\n",
    "plt.xlabel('Valor de k', fontsize=12)\n",
    "plt.ylabel('Exactitud', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_score = max(scores)\n",
    "best_k = k_range[scores.index(best_score)]\n",
    "\n",
    "#Mostrar información del mejor k\n",
    "print(f\"Mejor k encontrado: {best_k}\")\n",
    "print(f\"Exactitud correspondiente: {best_score:.4f}\")\n",
    "\n",
    "#Verificar si hay múltiples k con la misma exactitud máxima\n",
    "best_k_candidates = [k for k, score in zip(k_range, scores) if score == best_score]\n",
    "if len(best_k_candidates) > 1:\n",
    "    print(f\"Otros k con la misma exactitud: {best_k_candidates}\")\n",
    "    print(f\"Se eligió k={best_k} (el más pequeño para mayor simplicidad)\")\n",
    "\n",
    "#Entrena al modelo con el mejor K posible\n",
    "final_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn_model.fit(X_train_scaled, y_train_balanced)\n",
    "final_y_pred = final_knn_model.predict(X_test_scaled)\n",
    "\n",
    "#Métricas del modelo final\n",
    "final_accuracy_KNN = accuracy_score(y_test, final_y_pred)\n",
    "print(f\"\\nExactitud del modelo final: {final_accuracy_KNN:.4f}\")\n",
    "\n",
    "#Matriz de confusión para el modelo final.\n",
    "final_conf_matrix = confusion_matrix(y_test, final_y_pred, labels=[1, 0])\n",
    "\n",
    "#Visualizar la matriz de confusión final\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(final_conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Positivo', 'Negativo'],\n",
    "            yticklabels=['Positivo', 'Negativo'])\n",
    "plt.title(f'Matriz de Confusión - KNN (k={best_k})')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación final\n",
    "print(\"\\nReporte de clasificación final:\")\n",
    "print(classification_report(y_test, final_y_pred))\n",
    "\n",
    "precision_KNN = precision_score(y_test, y_pred)\n",
    "recall_KNN = recall_score(y_test, y_pred)\n",
    "f1_knn = f1_score(y_test, y_pred)\n",
    "tn_knn, fp_knn, fn_knn, tp_knn = final_conf_matrix.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "#Separar datos de entrenamiento y de testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Balancear conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Estandarizar data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Optimizar max_depth (profundidad del árbol)\n",
    "depth_range = range(1, 21)\n",
    "depth_scores = []\n",
    "\n",
    "for depth in depth_range:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = tree.predict(X_test_scaled)\n",
    "    depth_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Gráfico para max_depth\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(depth_range, depth_scores, marker='o', linewidth=2, markersize=4)\n",
    "plt.title('Exactitud vs Max Depth', fontsize=12)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(depth_range)\n",
    "\n",
    "#Optimizar min_samples_split (Valor mínimo para separar un nodo interno)\n",
    "split_range = range(2, 21)\n",
    "split_scores = []\n",
    "\n",
    "for split in split_range:\n",
    "    tree = DecisionTreeClassifier(min_samples_split=split, random_state=42)\n",
    "    tree.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = tree.predict(X_test_scaled)\n",
    "    split_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Gráfico para min_samples_split\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(split_range, split_scores, marker='o', linewidth=2, markersize=4, color='orange')\n",
    "plt.title('Exactitud vs Min Samples Split', fontsize=12)\n",
    "plt.xlabel('Min Samples Split')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(split_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Optimizar min_samples_leaf (Valor mínimo para ser un nodo hoja)\n",
    "leaf_range = range(1, 11)\n",
    "leaf_scores = []\n",
    "\n",
    "for leaf in leaf_range:\n",
    "    tree = DecisionTreeClassifier(min_samples_leaf=leaf, random_state=42)\n",
    "    tree.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = tree.predict(X_test_scaled)\n",
    "    leaf_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Gráfico para min_samples_leaf\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(leaf_range, leaf_scores, marker='o', linewidth=2, markersize=4, color='green')\n",
    "plt.title('Exactitud vs Min Samples Leaf', fontsize=12)\n",
    "plt.xlabel('Min Samples Leaf')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(leaf_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Mejor max_depth\n",
    "best_depth_score = max(depth_scores)\n",
    "best_depth = depth_range[depth_scores.index(best_depth_score)]\n",
    "#Mejor min_samples_split\n",
    "best_split_score = max(split_scores)\n",
    "best_split = split_range[split_scores.index(best_split_score)]\n",
    "#Mejor min_samples_leaf\n",
    "best_leaf_score = max(leaf_scores)\n",
    "best_leaf = leaf_range[leaf_scores.index(best_leaf_score)]\n",
    "\n",
    "#Modelo a entrenar\n",
    "final_tree = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_split=best_split,\n",
    "    min_samples_leaf=best_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "final_tree.fit(X_train_scaled, y_train_balanced)\n",
    "final_y_pred = final_tree.predict(X_test_scaled)\n",
    "\n",
    "#Métricas del modelo\n",
    "final_accuracy_TREE = accuracy_score(y_test, final_y_pred)\n",
    "precision_TREE = precision_score(y_test, y_pred)\n",
    "recall_TREE = recall_score(y_test, y_pred)\n",
    "f1_tree = f1_score(y_test, y_pred)\n",
    "tn_tree, fp_tree, fn_tree, tp_tree = final_conf_matrix.ravel()\n",
    "\n",
    "final_conf_matrix = confusion_matrix(y_test, final_y_pred, labels=[1, 0])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(final_conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Positivo', 'Negativo'],\n",
    "            yticklabels=['Positivo', 'Negativo'])\n",
    "plt.title(f'Matriz de Confusión - Árbol de Decisión\\n(depth={best_depth}, split={best_split}, leaf={best_leaf})')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación final\n",
    "print(\"\\nReporte de clasificación final:\")\n",
    "print(classification_report(y_test, final_y_pred, zero_division=0))\n",
    "\n",
    "plot_tree(final_tree,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "#Separar datos de entrenamiento y de testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Balancear conjunto de entrenamiento\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#Estandarizar data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Optimizar n_estimators\n",
    "range_estimators = range(10, 201, 20)\n",
    "score_estimators = []\n",
    "\n",
    "for estimator in range_estimators:\n",
    "    forest = RandomForestClassifier(n_estimators=estimator, random_state=42)\n",
    "    forest.fit(X_train_scaled, y_train_balanced) \n",
    "    y_pred = forest.predict(X_test_scaled)        \n",
    "    score_estimators.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Optimizar max_depth\n",
    "depth_range = [None] + list(range(5, 21, 2))\n",
    "depth_scores = []\n",
    "\n",
    "for depth in depth_range:\n",
    "    forest = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    forest.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = forest.predict(X_test_scaled)\n",
    "    depth_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Optimizar min_samples_split\n",
    "split_range = range(2, 11)\n",
    "split_scores = []\n",
    "\n",
    "for split in split_range:\n",
    "    forest = RandomForestClassifier(min_samples_split=split, random_state=42)\n",
    "    forest.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = forest.predict(X_test_scaled)\n",
    "    split_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Gráfico n_estimators\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range_estimators, score_estimators, marker='o', linewidth=2, markersize=4)\n",
    "plt.title('Exactitud vs N_Estimators', fontsize=12)\n",
    "plt.xlabel('N_Estimators')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(10, 201, 40))  # Mostrar cada 40\n",
    "#Gráfico max_depth\n",
    "plt.subplot(1, 3, 2)\n",
    "depth_labels = ['None'] + [str(d) for d in depth_range[1:]]\n",
    "plt.plot(range(len(depth_range)), depth_scores, marker='o', linewidth=2, markersize=4, color='orange')\n",
    "plt.title('Exactitud vs Max_Depth', fontsize=12)\n",
    "plt.xlabel('Max_Depth')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(len(depth_range)), depth_labels, rotation=45)\n",
    "#Gráfico min_samples_split\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(split_range, split_scores, marker='o', linewidth=2, markersize=4, color='green')\n",
    "plt.title('Exactitud vs Min_Samples_Split', fontsize=12)\n",
    "plt.xlabel('Min_Samples_Split')\n",
    "plt.ylabel('Exactitud')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(split_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Mejor n_estimators\n",
    "best_estimator_score = max(score_estimators)\n",
    "best_estimator = list(range_estimators)[score_estimators.index(best_estimator_score)]\n",
    "#Mejor max_depth\n",
    "best_depth_score = max(depth_scores)\n",
    "best_depth = depth_range[depth_scores.index(best_depth_score)]\n",
    "#Mejor min_samples_split\n",
    "best_split_score = max(split_scores)\n",
    "best_split = split_range[split_scores.index(best_split_score)]\n",
    "\n",
    "final_forest = RandomForestClassifier(\n",
    "    n_estimators=best_estimator,\n",
    "    max_depth=best_depth,\n",
    "    min_samples_split=best_split,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_forest.fit(X_train_scaled, y_train_balanced)\n",
    "final_y_pred = final_forest.predict(X_test_scaled)\n",
    "\n",
    "#Métricas del modelo final\n",
    "final_accuracy_RF = accuracy_score(y_test, final_y_pred)\n",
    "print(f\"\\nExactitud del modelo final: {final_accuracy_RF:.4f}\")\n",
    "\n",
    "#Matriz de confusión\n",
    "final_conf_matrix = confusion_matrix(y_test, final_y_pred, labels=[1, 0])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(final_conf_matrix, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Positivo', 'Negativo'],\n",
    "            yticklabels=['Positivo', 'Negativo'])\n",
    "plt.title(f'Matriz de Confusión - Random Forest\\n(estimators={best_estimator}, depth={best_depth}, split={best_split})')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predicho')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación final\n",
    "print(\"\\nReporte de clasificación final:\")\n",
    "print(classification_report(y_test, final_y_pred, zero_division=0))\n",
    "\n",
    "precision_RF = precision_score(y_test, y_pred)\n",
    "recall_RF = recall_score(y_test, y_pred)\n",
    "f1_rf = f1_score(y_test, y_pred)\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = final_conf_matrix.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvas ROC de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#Crear la gráfica de la curva ROC de Gaussian Naive Bayes \n",
    "y_pred_proba = nb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_NB = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_NB:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Gaussian Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#Crear la gráfica de la curva ROC de Bernoulli Naive Bayes\n",
    "y_pred_proba = Bernoulli_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_BBN = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_BBN:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Bernoulli Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#Crear la gráfica de la curva ROC de KNN\n",
    "y_pred_proba = final_knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_KNN = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_KNN:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - KNN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#Crear la gráfica de la curva ROC de Árbol de decisión\n",
    "y_pred_proba = final_tree.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_TREE= roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_TREE:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Árbol de decisión')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#Crear la gráfica de la curva ROC de Random Forest\n",
    "y_pred_proba = final_forest.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_RF = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_RF:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#Crear la gráfica de la curva ROC de Multinomial Naive Bayes\n",
    "y_pred_proba = multinomial_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score_multinomial = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc_score_multinomial:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Línea de referencia (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Multinomial Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de los valores importantes de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_Bayes_dict = {\n",
    "    'Accuracy': accuracy_NB,\n",
    "    'Precision': precision_NB,\n",
    "    'Recall': recall_NB,\n",
    "    'F1-Score': f1_nb,\n",
    "    'True Positives': tp_nb,\n",
    "    'True Negatives': tn_nb,\n",
    "    'False Positives': fp_nb,\n",
    "    'False Negatives': fn_nb,\n",
    "    'AUC-score': auc_score_NB\n",
    "}\n",
    "Naive_Bayes_dict2 = {\n",
    "    'Accuracy': accuracy_BNB,\n",
    "    'Precision': precision_BNB,\n",
    "    'Recall': recall_BNB,\n",
    "    'F1-Score': f1_bnb,\n",
    "    'True Positives': tp_bnb,\n",
    "    'True Negatives': tn_bnb,\n",
    "    'False Positives': fp_bnb,\n",
    "    'False Negatives': fn_bnb,\n",
    "    'AUC-score': auc_score_BBN\n",
    "}\n",
    "\n",
    "KNN = {\n",
    "    'Accuracy': final_accuracy_KNN,\n",
    "    'Precision': precision_KNN,\n",
    "    'Recall': recall_KNN,\n",
    "    'F1-Score': f1_knn,\n",
    "    'True Positives': tp_knn,\n",
    "    'True Negatives': tn_knn,\n",
    "    'False Positives': fp_knn,\n",
    "    'False Negatives': fn_knn,\n",
    "    'AUC-score': auc_score_KNN\n",
    "}\n",
    "TREE = {\n",
    "    'Accuracy': final_accuracy_TREE,\n",
    "    'Precision': precision_TREE,\n",
    "    'Recall': recall_TREE,\n",
    "    'F1-Score': f1_tree,\n",
    "    'True Positives': tp_tree,\n",
    "    'True Negatives': tn_tree,\n",
    "    'False Positives': fp_tree,\n",
    "    'False Negatives': fn_tree,\n",
    "    'AUC-score': auc_score_TREE,\n",
    "}\n",
    "RF = {\n",
    "    'Accuracy': final_accuracy_RF,\n",
    "    'Precision': precision_RF,\n",
    "    'Recall': recall_RF,\n",
    "    'F1-Score': f1_rf,\n",
    "    'True Positives': tp_rf,\n",
    "    'True Negatives': tn_rf,\n",
    "    'False Positives': fp_rf,\n",
    "    'False Negatives': fn_rf,\n",
    "    'AUC-score': auc_score_RF\n",
    "}\n",
    "Multinomial = {\n",
    "    'Accuracy': accuracy_multinomial,\n",
    "    'Precision': precision_multinomial,\n",
    "    'Recall': recall_multinomial,\n",
    "    'F1-Score': f1_multinomial,\n",
    "    'True Positives': tp_multinomial,\n",
    "    'True Negatives': tn_multinomial,\n",
    "    'False Positives': fp_multinomial,\n",
    "    'False Negatives': fn_multinomial,\n",
    "    'AUC-score': auc_score_multinomial\n",
    "}\n",
    "\n",
    "resume = pd.DataFrame({\n",
    "    'Gaussian Naive Bayes': pd.Series(Naive_Bayes_dict),\n",
    "    'Bernoulli Naive Bayes': pd.Series(Naive_Bayes_dict2),\n",
    "    'Multinomial Naive Bayes': pd.Series(Multinomial),\n",
    "    'KNN': pd.Series(KNN),\n",
    "    'Decision Tree': pd.Series(TREE),\n",
    "    'Random Forest': pd.Series(RF)\n",
    "})\n",
    "\n",
    "resume.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos por métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_grouped_metrics(df):\n",
    "    \"\"\"Crear un gráfico de barras separado para cada métrica\"\"\"\n",
    "    metrics = df.index.tolist()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        #Crear una nueva figura para cada métrica\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        #Datos para la métrica actual\n",
    "        values = df.loc[metric].values\n",
    "        models = df.columns.tolist()\n",
    "        \n",
    "        bars = plt.bar(models, values, alpha=0.8)\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        for j, (model, value) in enumerate(zip(models, values)):\n",
    "            if pd.notna(value):  # Solo si el valor no es NaN\n",
    "                plt.text(j, value + max(values)*0.01, f'{value:.3f}',\n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title(f'{metric}', fontweight='bold', fontsize=16)\n",
    "        plt.ylabel('Valor', fontsize=12)\n",
    "        plt.xlabel('Modelos', fontsize=12)\n",
    "        plt.ylim(0, max(values) * 1.1 if max(values) > 0 else 1)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_grouped_metrics(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración con semillas distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "# Inicializar listas para almacenar métricas\n",
    "accuracy_list_NB = []\n",
    "precision_list_NB = []\n",
    "recall_list_NB = []\n",
    "f1_list_NB = []\n",
    "tn_list_NB = []\n",
    "fp_list_NB = []\n",
    "fn_list_NB = []  # Corregí el nombre de la variable (tenías fn_bnb_list_NB)\n",
    "tp_list_NB = []\n",
    "\n",
    "print(\"Ejecutando evaluaciones con diferentes random states...\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"Iteración {i}/10\")\n",
    "    \n",
    "    # División de datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Balancear el conjunto de entrenamiento\n",
    "    ros = RandomOverSampler(random_state=i)\n",
    "    X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Escalar datos\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entrenamiento del modelo\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = nb_model.predict(X_test_scaled)\n",
    "    \n",
    "    #Calcular métricas\n",
    "    accuracy_NB = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "    precision_NB = precision_score(y_test, y_pred)\n",
    "    recall_NB = recall_score(y_test, y_pred)\n",
    "    f1_nb = f1_score(y_test, y_pred)\n",
    "    tn_nb, fp_nb, fn_nb, tp_nb = conf_matrix.ravel()\n",
    "    \n",
    "    # Agregar valores a las listas\n",
    "    accuracy_list_NB.append(accuracy_NB)\n",
    "    precision_list_NB.append(precision_NB)\n",
    "    recall_list_NB.append(recall_NB)\n",
    "    f1_list_NB.append(f1_nb)\n",
    "    tn_list_NB.append(tn_nb)\n",
    "    fp_list_NB.append(fp_nb)\n",
    "    fn_list_NB.append(fn_nb)\n",
    "    tp_list_NB.append(tp_nb)\n",
    "    \n",
    "    # Reporte de clasificación\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Reporte de clasificación - Iteración {i}:\")\n",
    "    print(class_report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "metrics_data = {\n",
    "    'Accuracy': accuracy_list_NB,\n",
    "    'Precision': precision_list_NB,\n",
    "    'Recall': recall_list_NB,\n",
    "    'F1-Score': f1_list_NB\n",
    "}\n",
    "\n",
    "for metric_name, metric_values in metrics_data.items():\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "    min_val = np.min(metric_values)\n",
    "    max_val = np.max(metric_values)\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Media: {mean_val:.4f}\")\n",
    "    print(f\"  Desviación Estándar: {std_val:.4f}\")\n",
    "    print(f\"  Mínimo: {min_val:.4f}\")\n",
    "    print(f\"  Máximo: {max_val:.4f}\")\n",
    "    print(f\"  Valores: {[round(x, 4) for x in metric_values]}\")\n",
    "\n",
    "# Mostrar estadísticas de la matriz de confusión\n",
    "print(f\"\\nMatriz de Confusión (promedios):\")\n",
    "print(f\"  TP (Verdaderos Positivos): {np.mean(tp_list_NB):.2f} ± {np.std(tp_list_NB):.2f}\")\n",
    "print(f\"  TN (Verdaderos Negativos): {np.mean(tn_list_NB):.2f} ± {np.std(tn_list_NB):.2f}\")\n",
    "print(f\"  FP (Falsos Positivos): {np.mean(fp_list_NB):.2f} ± {np.std(fp_list_NB):.2f}\")\n",
    "print(f\"  FN (Falsos Negativos): {np.mean(fn_list_NB):.2f} ± {np.std(fn_list_NB):.2f}\")\n",
    "\n",
    "# Gráfico de las métricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "means = [np.mean(values) for values in metrics_data.values()]\n",
    "stds = [np.std(values) for values in metrics_data.values()]\n",
    "\n",
    "bars = plt.bar(x_pos, means, yerr=stds, capsize=10, alpha=0.7, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'plum'],\n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
    "             f'{mean:.3f}±{std:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Rendimiento del Modelo Gaussian Naive Bayes\\n(Media ± Desviación Estándar)')\n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Árboles de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "# Inicializar listas para almacenar métricas\n",
    "accuracy_list_tree = []\n",
    "precision_list_tree = []\n",
    "recall_list_tree = []\n",
    "f1_list_tree = []\n",
    "tn_list_tree = []\n",
    "fp_list_tree = []\n",
    "fn_list_tree = []\n",
    "tp_list_tree = []\n",
    "\n",
    "print(\"Ejecutando evaluaciones Decision Tree con diferentes random states...\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"Iteración {i}/10\")\n",
    "    \n",
    "    # Separar datos de entrenamiento y de testeo (CORREGIDO: usar i como random_state)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Balancear conjunto de entrenamiento\n",
    "    ros = RandomOverSampler(random_state=i)\n",
    "    X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Estandarizar data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # CORREGIDO: Crear, entrenar y predecir con el modelo\n",
    "    tree_model = DecisionTreeClassifier(max_depth=8, min_samples_split=2, min_samples_leaf=1, random_state=i)\n",
    "    tree_model.fit(X_train_scaled, y_train_balanced)  \n",
    "    y_pred = tree_model.predict(X_test_scaled)        \n",
    "    \n",
    "    # Calcular métricas (CORREGIDO: orden y variables correctas)\n",
    "    accuracy_tree = accuracy_score(y_test, y_pred)\n",
    "    precision_tree = precision_score(y_test, y_pred)\n",
    "    recall_tree = recall_score(y_test, y_pred)\n",
    "    f1_tree = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "    tn_tree, fp_tree, fn_tree, tp_tree = conf_matrix.ravel()\n",
    "    \n",
    "    # Agregar valores a las listas\n",
    "    accuracy_list_tree.append(accuracy_tree)\n",
    "    precision_list_tree.append(precision_tree)\n",
    "    recall_list_tree.append(recall_tree)              # CORREGIDO: variable correcta\n",
    "    f1_list_tree.append(f1_tree)\n",
    "    tn_list_tree.append(tn_tree)\n",
    "    fp_list_tree.append(fp_tree)\n",
    "    fn_list_tree.append(fn_tree)\n",
    "    tp_list_tree.append(tp_tree)\n",
    "    \n",
    "    # Mostrar métricas de esta iteración\n",
    "    print(f\"  Accuracy: {accuracy_tree:.4f}\")\n",
    "    print(f\"  Precision: {precision_tree:.4f}\")\n",
    "    print(f\"  Recall: {recall_tree:.4f}\")\n",
    "    print(f\"  F1-Score: {f1_tree:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"¡Evaluaciones completadas!\")\n",
    "\n",
    "# MOVIDO FUERA DEL BUCLE: Estadísticas finales\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESTADÍSTICAS DE LAS MÉTRICAS - DECISION TREE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_data = {\n",
    "    'Accuracy': accuracy_list_tree,\n",
    "    'Precision': precision_list_tree,\n",
    "    'Recall': recall_list_tree,\n",
    "    'F1-Score': f1_list_tree\n",
    "}\n",
    "\n",
    "for metric_name, metric_values in metrics_data.items():\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "    min_val = np.min(metric_values)\n",
    "    max_val = np.max(metric_values)\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Media: {mean_val:.4f}\")\n",
    "    print(f\"  Desviación Estándar: {std_val:.4f}\")\n",
    "    print(f\"  Mínimo: {min_val:.4f}\")\n",
    "    print(f\"  Máximo: {max_val:.4f}\")\n",
    "    print(f\"  Valores: {[round(x, 4) for x in metric_values]}\")\n",
    "\n",
    "# Mostrar estadísticas de la matriz de confusión\n",
    "print(f\"\\nMatriz de Confusión (promedios):\")\n",
    "print(f\"  TP (Verdaderos Positivos): {np.mean(tp_list_tree):.2f} ± {np.std(tp_list_tree):.2f}\")\n",
    "print(f\"  TN (Verdaderos Negativos): {np.mean(tn_list_tree):.2f} ± {np.std(tn_list_tree):.2f}\")\n",
    "print(f\"  FP (Falsos Positivos): {np.mean(fp_list_tree):.2f} ± {np.std(fp_list_tree):.2f}\")\n",
    "print(f\"  FN (Falsos Negativos): {np.mean(fn_list_tree):.2f} ± {np.std(fn_list_tree):.2f}\")\n",
    "\n",
    "# Gráfico de las métricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "means = [np.mean(values) for values in metrics_data.values()]\n",
    "stds = [np.std(values) for values in metrics_data.values()]\n",
    "\n",
    "bars = plt.bar(x_pos, means, yerr=stds, capsize=10, alpha=0.7,\n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'plum'],\n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
    "             f'{mean:.3f}±{std:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Rendimiento del Modelo Decision Tree\\n(Media ± Desviación Estándar)') \n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "# Inicializar listas para almacenar métricas\n",
    "accuracy_list_KNN = []\n",
    "precision_list_KNN = []\n",
    "recall_list_KNN = []\n",
    "f1_list_KNN = []\n",
    "tn_list_KNN = []\n",
    "fp_list_KNN = []\n",
    "fn_list_KNN = []\n",
    "tp_list_KNN = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"Iteración {i}/10\")\n",
    "    \n",
    "    # División de datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Balancear conjunto de entrenamiento\n",
    "    ros = RandomOverSampler(random_state=i)\n",
    "    X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Estandarizar data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entrenar el modelo con el mejor K posible\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=30)\n",
    "    knn_model.fit(X_train_scaled, y_train_balanced)\n",
    "    y_pred = knn_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy_KNN = accuracy_score(y_test, y_pred)\n",
    "    precision_KNN = precision_score(y_test, y_pred)\n",
    "    recall_KNN = recall_score(y_test, y_pred)\n",
    "    f1_knn = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "    tn_knn, fp_knn, fn_knn, tp_knn = conf_matrix.ravel()\n",
    "    \n",
    "    # Agregar valores a las listas\n",
    "    accuracy_list_KNN.append(accuracy_KNN)\n",
    "    precision_list_KNN.append(precision_KNN)\n",
    "    recall_list_KNN.append(recall_KNN)\n",
    "    f1_list_KNN.append(f1_knn)\n",
    "    tn_list_KNN.append(tn_knn)\n",
    "    fp_list_KNN.append(fp_knn)\n",
    "    fn_list_KNN.append(fn_knn)\n",
    "    tp_list_KNN.append(tp_knn)\n",
    "    \n",
    "    # Mostrar métricas de esta iteración\n",
    "    print(f\"  Accuracy: {accuracy_KNN:.4f}\")\n",
    "    print(f\"  Precision: {precision_KNN:.4f}\")\n",
    "    print(f\"  Recall: {recall_KNN:.4f}\")\n",
    "    print(f\"  F1-Score: {f1_knn:.4f}\")\n",
    "\n",
    "\n",
    "metrics_data = {\n",
    "    'Accuracy': accuracy_list_KNN,\n",
    "    'Precision': precision_list_KNN,\n",
    "    'Recall': recall_list_KNN,\n",
    "    'F1-Score': f1_list_KNN\n",
    "}\n",
    "\n",
    "\n",
    "# Gráfico de las métricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "means = [np.mean(values) for values in metrics_data.values()]\n",
    "stds = [np.std(values) for values in metrics_data.values()]\n",
    "\n",
    "bars = plt.bar(x_pos, means, yerr=stds, capsize=10, alpha=0.7,\n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'plum'],\n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
    "             f'{mean:.3f}±{std:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Rendimiento del Modelo KNN (k=30)\\n(Media ± Desviación Estándar)')\n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "# Listas para guardar las métricas\n",
    "accuracy_list_RF = []\n",
    "precision_list_RF = []\n",
    "recall_list_RF = []\n",
    "f1_list_RF = []\n",
    "tn_list_RF = []\n",
    "fp_list_RF = []\n",
    "fn_list_RF = []\n",
    "tp_list_RF = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(f\"\\nIteración {i}:\")\n",
    "    \n",
    "    # Separar datos de entrenamiento y de testeo\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Balancear conjunto de entrenamiento\n",
    "    ros = RandomOverSampler(random_state=i)\n",
    "    X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Estandarizar data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    final_forest = RandomForestClassifier(random_state=i)\n",
    "    final_forest.fit(X_train_scaled, y_train_balanced)\n",
    "    final_y_pred = final_forest.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    final_accuracy_RF = accuracy_score(y_test, final_y_pred)\n",
    "    precision_RF = precision_score(y_test, final_y_pred, zero_division=0)\n",
    "    recall_RF = recall_score(y_test, final_y_pred, zero_division=0)\n",
    "    f1_rf = f1_score(y_test, final_y_pred, zero_division=0)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    final_conf_matrix = confusion_matrix(y_test, final_y_pred, labels=[1, 0])\n",
    "    tn_rf, fp_rf, fn_rf, tp_rf = final_conf_matrix.ravel()\n",
    "    \n",
    "    # Guardar métricas en las listas\n",
    "    accuracy_list_RF.append(final_accuracy_RF)\n",
    "    precision_list_RF.append(precision_RF)\n",
    "    recall_list_RF.append(recall_RF)\n",
    "    f1_list_RF.append(f1_rf)\n",
    "    tn_list_RF.append(tn_rf)\n",
    "    fp_list_RF.append(fp_rf)\n",
    "    fn_list_RF.append(fn_rf)\n",
    "    tp_list_RF.append(tp_rf)\n",
    "    \n",
    "    print(f\"Exactitud: {final_accuracy_RF:.4f}\")\n",
    "    print(f\"Precisión: {precision_RF:.4f}\")\n",
    "    print(f\"Recall: {recall_RF:.4f}\")\n",
    "    print(f\"F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "metrics_data = {\n",
    "    'Accuracy': accuracy_list_RF,\n",
    "    'Precision': precision_list_RF,\n",
    "    'Recall': recall_list_RF,\n",
    "    'F1-Score': f1_list_RF\n",
    "}\n",
    "\n",
    "\n",
    "# Gráfico de las métricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "means = [np.mean(values) for values in metrics_data.values()]\n",
    "stds = [np.std(values) for values in metrics_data.values()]\n",
    "\n",
    "bars = plt.bar(x_pos, means, yerr=stds, capsize=10, alpha=0.7,\n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'plum'],\n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
    "             f'{mean:.3f}±{std:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Rendimiento del Modelo RF\\n(Media ± Desviación Estándar)')\n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación de la última iteración\n",
    "print(\"\\nReporte de clasificación (última iteración):\")\n",
    "print(classification_report(y_test, final_y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos no supervisados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DISTANCIAS AL CENTROIDE ===\n",
      "Distancia promedio (Train): 5.489\n",
      "Distancia promedio (Test): 5.482\n",
      "Desviación estándar (Train): 0.645\n",
      "Desviación estándar (Test): 0.624\n",
      "\n",
      "=== RESUMEN BASADO EN DISTANCIAS AL CENTROIDE ===\n",
      "\n",
      "Cluster 0:\n",
      "  - Número de puntos: 57\n",
      "  - Distancia promedio: 5.577\n",
      "  - Distancia mínima: 4.191\n",
      "  - Distancia máxima: 7.192\n",
      "  - Desviación estándar: 0.646\n",
      "\n",
      "Cluster 1:\n",
      "  - Número de puntos: 165\n",
      "  - Distancia promedio: 5.532\n",
      "  - Distancia mínima: 4.013\n",
      "  - Distancia máxima: 7.186\n",
      "  - Desviación estándar: 0.633\n",
      "\n",
      "Cluster 2:\n",
      "  - Número de puntos: 61\n",
      "  - Distancia promedio: 5.411\n",
      "  - Distancia mínima: 4.078\n",
      "  - Distancia máxima: 6.990\n",
      "  - Desviación estándar: 0.551\n",
      "\n",
      "Cluster 3:\n",
      "  - Número de puntos: 147\n",
      "  - Distancia promedio: 5.417\n",
      "  - Distancia mínima: 3.799\n",
      "  - Distancia máxima: 6.784\n",
      "  - Desviación estándar: 0.623\n",
      "Distancia entre centroides: 2.991\n",
      "Silhouette Score: 0.030\n",
      "=== DIAGNÓSTICO DEL SILHOUETTE SCORE ===\n",
      "Silhouette Score promedio: 0.030\n",
      "Rango del Silhouette Score: [-0.009, 0.088]\n",
      "\n",
      "Cluster 0:\n",
      "  - Silhouette promedio: 0.038\n",
      "  - Puntos con score negativo: 0/267\n",
      "  - Puntos mal clasificados (%): 0.0%\n",
      "\n",
      "Cluster 1:\n",
      "  - Silhouette promedio: 0.018\n",
      "  - Puntos con score negativo: 88/586\n",
      "  - Puntos mal clasificados (%): 15.0%\n",
      "\n",
      "Cluster 2:\n",
      "  - Silhouette promedio: 0.043\n",
      "  - Puntos con score negativo: 0/228\n",
      "  - Puntos mal clasificados (%): 0.0%\n",
      "\n",
      "Cluster 3:\n",
      "  - Silhouette promedio: 0.033\n",
      "  - Puntos con score negativo: 0/638\n",
      "  - Puntos mal clasificados (%): 0.0%\n",
      "\n",
      "=== SEPARACIÓN DE CLUSTERS ===\n",
      "Distancia entre centroides: 2.991\n",
      "Distancia promedio intra-cluster: 5.489\n",
      "Ratio separación/compacidad: 0.545\n",
      "\n",
      "=== TAMAÑOS DE CLUSTERS ===\n",
      "Cluster 1: 586 puntos (34.1%)\n",
      "Cluster 3: 638 puntos (37.1%)\n",
      "Cluster 0: 267 puntos (15.5%)\n",
      "Cluster 2: 228 puntos (13.3%)\n",
      "  K=2: Silhouette = 0.023\n",
      "  K=3: Silhouette = 0.029\n",
      "  K=4: Silhouette = 0.030\n",
      "\n",
      "Mejor K según Silhouette: 4 (Score: 0.030)\n",
      "CH score: 45.867\n",
      "DB score: 4.921\n",
      "Adjusted Rand Index: -0.011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from collections import Counter\n",
    "\n",
    "# Separar variable objetivo de los demás atributos\n",
    "X = archivo.drop('Diagnosis', axis=1)\n",
    "y = archivo['Diagnosis']\n",
    "\n",
    "# División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Estandarizar data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar el modelo final con K óptimo\n",
    "best_kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters_train = best_kmeans.fit_predict(X_train_scaled)\n",
    "clusters_test = best_kmeans.predict(X_test_scaled)\n",
    "\n",
    "# Obtener centroides\n",
    "centroids = best_kmeans.cluster_centers_\n",
    "\n",
    "# Calcular distancias al centroide para cada punto\n",
    "def calculate_centroid_distances(X, clusters, centroids):\n",
    "    \"\"\"Calcula la distancia de cada punto a su centroide asignado\"\"\"\n",
    "    distances = []\n",
    "    for i, point in enumerate(X):\n",
    "        cluster_id = clusters[i]\n",
    "        centroid = centroids[cluster_id]\n",
    "        distance = np.linalg.norm(point - centroid)\n",
    "        distances.append(distance)\n",
    "    return np.array(distances)\n",
    "\n",
    "# Calcular distancias para train y test\n",
    "distances_train = calculate_centroid_distances(X_train_scaled, clusters_train, centroids)\n",
    "distances_test = calculate_centroid_distances(X_test_scaled, clusters_test, centroids)\n",
    "\n",
    "# Estadísticas de distancias\n",
    "print(f\"\\n=== DISTANCIAS AL CENTROIDE ===\")\n",
    "print(f\"Distancia promedio (Train): {np.mean(distances_train):.3f}\")\n",
    "print(f\"Distancia promedio (Test): {np.mean(distances_test):.3f}\")\n",
    "print(f\"Desviación estándar (Train): {np.std(distances_train):.3f}\")\n",
    "print(f\"Desviación estándar (Test): {np.std(distances_test):.3f}\")\n",
    "\n",
    "distance = np.linalg.norm(centroids[0] - centroids[1])\n",
    "sh_score = silhouette_score(X_train_scaled, clusters_train)\n",
    "# Resumen final con enfoque en distancias\n",
    "print(\"\\n=== RESUMEN BASADO EN DISTANCIAS AL CENTROIDE ===\")\n",
    "for cluster_id in np.unique(clusters_test):\n",
    "    mask = clusters_test == cluster_id\n",
    "    cluster_distances = distances_test[mask]\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  - Número de puntos: {len(cluster_distances)}\")\n",
    "    print(f\"  - Distancia promedio: {np.mean(cluster_distances):.3f}\")\n",
    "    print(f\"  - Distancia mínima: {np.min(cluster_distances):.3f}\")\n",
    "    print(f\"  - Distancia máxima: {np.max(cluster_distances):.3f}\")\n",
    "    print(f\"  - Desviación estándar: {np.std(cluster_distances):.3f}\")\n",
    "\n",
    "print(f\"Distancia entre centroides: {distance:.3f}\")\n",
    "print(f\"Silhouette Score: {sh_score:.3f}\")\n",
    "\n",
    "# 1. Análisis detallado del Silhouette Score\n",
    "silhouette_samples_scores = silhouette_samples(X_train_scaled, clusters_train)\n",
    "print(\"=== DIAGNÓSTICO DEL SILHOUETTE SCORE ===\")\n",
    "print(f\"Silhouette Score promedio: {sh_score:.3f}\")\n",
    "print(f\"Rango del Silhouette Score: [{np.min(silhouette_samples_scores):.3f}, {np.max(silhouette_samples_scores):.3f}]\")\n",
    "\n",
    "# Análisis por cluster\n",
    "for cluster_id in np.unique(clusters_train):\n",
    "    cluster_silhouette_scores = silhouette_samples_scores[clusters_train == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  - Silhouette promedio: {np.mean(cluster_silhouette_scores):.3f}\")\n",
    "    print(f\"  - Puntos con score negativo: {np.sum(cluster_silhouette_scores < 0)}/{len(cluster_silhouette_scores)}\")\n",
    "    print(f\"  - Puntos mal clasificados (%): {(np.sum(cluster_silhouette_scores < 0)/len(cluster_silhouette_scores)*100):.1f}%\")\n",
    "\n",
    "# 2. Verificar separación de clusters\n",
    "print(f\"\\n=== SEPARACIÓN DE CLUSTERS ===\")\n",
    "print(f\"Distancia entre centroides: {distance:.3f}\")\n",
    "print(f\"Distancia promedio intra-cluster: {np.mean(distances_train):.3f}\")\n",
    "print(f\"Ratio separación/compacidad: {distance/np.mean(distances_train):.3f}\")\n",
    "\n",
    "# 3. Análisis de tamaños de clusters\n",
    "cluster_sizes = Counter(clusters_train)\n",
    "print(f\"\\n=== TAMAÑOS DE CLUSTERS ===\")\n",
    "for cluster_id, size in cluster_sizes.items():\n",
    "    percentage = (size / len(clusters_train)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {size} puntos ({percentage:.1f}%)\")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "K_range = range(2, min(8, len(np.unique(y_train)) + 3))\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42)\n",
    "    temp_clusters = kmeans_temp.fit_predict(X_train_scaled)\n",
    "    temp_score = silhouette_score(X_train_scaled, temp_clusters)\n",
    "    silhouette_scores.append(temp_score)\n",
    "    print(f\"  K={k}: Silhouette = {temp_score:.3f}\")\n",
    "\n",
    "print(f\"\\nMejor K según Silhouette: {K_range[np.argmax(silhouette_scores)]} (Score: {max(silhouette_scores):.3f})\")\n",
    "ch_score = calinski_harabasz_score(X_train_scaled, clusters_train) #Razón entre dispersión entre grupos y la dispersión dentro del grupo.\n",
    "db_score = davies_bouldin_score(X_train_scaled, clusters_train) #Compara la similaridad entre cada cluster y el vecino más cercano, Mide que tan separados y compactos son.\n",
    "ari_score = adjusted_rand_score(y_test, clusters_test) #Similaridad entre dos clusters\n",
    "print(f\"CH score: {ch_score:.3f}\")\n",
    "print(f\"DB score: {db_score:.3f}\")\n",
    "print(f\"Adjusted Rand Index: {ari_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de reglas de asociación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
